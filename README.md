ğŸ“Œ AirPointer â€” AI Hand Gesture YouTube Controller

Transform the way you watch YouTube using AI-powered hand gestures.
Built with MediaPipe + OpenCV + PyAutoGUI, AirPointer lets you control YouTube touch-free â€” play, pause, adjust volume, skip videos, seek forward/backward, and more.

<p align="center"> <img src="https://img.shields.io/badge/Python-3.10%2B-blue" /> <img src="https://img.shields.io/badge/OpenCV-4.x-green" /> <img src="https://img.shields.io/badge/MediaPipe-0.10.x-red" /> <img src="https://img.shields.io/badge/Status-Active-brightgreen" /> </p>
 ğŸš€ Features

âœ” Real-time hand tracking (MediaPipe)
âœ” Low-latency gesture detection
âœ” Auto-opens YouTube on launch
âœ” Play / Pause (fist / open palm)
âœ” Volume Up/Down
âœ” Next / Previous video
âœ” Swipe gestures â†’ Seek forward/back
âœ” Thumb Up/Down gestures â†’ Volume control
âœ” Fist hold â†’ Mute toggle
âœ” Fullscreen toggle
âœ” Works on Windows, macOS, Linux

ğŸ¥ Demo
![Gesture Diagram](assets/gestures.png)


ğŸ–ï¸ Gesture Controls
Gesture	Action
âœŠ Fist	Pause
ğŸ–ï¸ All fingers up	Play
âœŒï¸ Index + Middle	Volume Up
ğŸ¤˜ Ring + Pinky	Volume Down
ğŸ¤™ Pinky only	Next Video
ğŸ‘ Thumb Up	Volume Up
ğŸ‘ Thumb Down	Volume Down
âœŠ (Hold 0.6s)	Mute / Unmute
ğŸ‘‰ Swipe Right	Seek Forward (10s)
ğŸ‘ˆ Swipe Left	Seek Back (10s)
F key (manual)	Fullscreen

ğŸ› ï¸ Tech Stack

Python 3.10+

OpenCV â†’ video processing

MediaPipe Hands â†’ gesture recognition

PyAutoGUI â†’ simulate keyboard actions

NumPy â†’ math utilities

ğŸ“¦ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/charishmasai99/airpointer.git
cd airpointer

2ï¸âƒ£ Install dependencies
pip install opencv-python mediapipe numpy pyautogui

3ï¸âƒ£ Run the application

python airpointer_youtube.py

âš™ï¸ Project Structure
AirPointer/
â”‚â”€â”€ airpointer_youtube.py      # Main application
â”‚â”€â”€ README.md                   # Project documentation
â”‚â”€â”€ .gitignore                  # Ignored files
â”‚â”€â”€ assets/                     # Images, demos, optional
â””â”€â”€ LICENSE           


ğŸ“Œ How It Works
The system uses a lightweight MediaPipe model to detect:

Finger positions

Landmark coordinates

Movement direction

Then these signals are converted into gesture actions mapped to YouTubeâ€™s built-in keyboard shortcuts.

ğŸ“ˆ Performance Optimizations

âœ” Processes reduced-size video frame for faster inference
âœ” Low model complexity = instant response
âœ” Lightweight gesture logic
âœ” Cooldowns to prevent multiple triggers

ğŸ›¡ï¸ Requirements

Windows / macOS / Linux

Python 3.10 or above

Webcam

Stable lighting for good detection

ğŸ“œ License

MIT License
You are free to use, modify, and distribute.

ğŸ™Œ Contributing

Pull requests are welcome!
If you want to add:

New gestures

Web UI

Hand-detection improvements

Voice prompts

Feel free to open an issue.

â­ Support

If you like this project, consider giving it a â­ on GitHub â€” it helps a lot!